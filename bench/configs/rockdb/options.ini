[Version]
  rocksdb_version=5.17.2
  options_file_version=1.1

[DBOptions]
 # When DB::Put is called, the data is written to both memtable (to be flushed to SST files later) and the WAL (write-ahead log) if it is enabled. 
 # In the case of a crash, RocksDB can recover as much as the memtable state that is reflected into the WAL. By default RocksDB automatically flushes 
 # the WAL from the application memory to the OS buffer after each ::Put. It however can be configured to perform the flush manually after an explicit call to ::FlushWAL.
 # Not doing fwrite syscall after each ::Put offers a tradeoff between reliability and write latency for the general case. As we explain below, some applications such as 
 # MyRocks benefit from this API to gain higher write throughput with however no compromise in reliability.
  manual_wal_flush=false

  # File ingestion behind 
  allow_ingest_behind=false

  # Speeds up DB shutdown by not flushing unpersisted data.
  avoid_flush_during_shutdown=false
  
  # Speeds up DB recovery by not flushing unpersisted data.
  avoid_flush_during_recovery=false
  info_log_level=INFO_LEVEL
  
  # normal none sequential willneed
  access_hint_on_compaction_start=NORMAL
  
  # The maximum number of microseconds that a write operation will use a yielding spin loop to coordinate with other write threads before blocking on a mutex.
  write_thread_max_yield_usec=100
  # The latency in microseconds after which a std::this_thread::yield call (sched_yield on Linux) is considered to be a signal that other processes or threads would like to use the current core.
  write_thread_slow_yield_usec=3

  # Recovery mode to control the consistency while replaying WAL Default:
  wal_recovery_mode=kPointInTimeRecovery

  # If true, allow multi-writers to update mem tables in parallel.
  allow_concurrent_memtable_write=true

  # Returns true if pipelined writes are enabled.
  enable_pipelined_write=false

  # If true, then DB::Open / CreateColumnFamily / DropColumnFamily / SetOptions will fail if options file is not detected or properly persisted.
  fail_if_options_file_error=false
  
  # If not zero, dump rocksdb.stats to LOG every stats_dump_period_sec Default: 600 (10 minutes)
  stats_dump_period_sec=600

  # Same as MutableDBOptionsInterface.setBytesPerSync(long) , but applies to WAL files Default: 0, turned off
  wal_bytes_per_sync=0

  # Once write-ahead logs exceed this size, we will start forcing the flush of column families whose memtables are backed by the oldest live WAL file.
  max_total_wal_size=0

  # The limited write rate to DB if ColumnFamilyOptions.softPendingCompactionBytesLimit() or ColumnFamilyOptions.level0SlowdownWritesTrigger() is triggered, or we 
  # are writing to the last mem table allowed and we allow more than 3 mem tables. It is calculated using size of user write requests before compression.
  # RocksDB may decide to slow down more if the compaction still gets behind further. Unit: bytes per second. Default: 16MB/s
  delayed_write_rate=16777216
  
  # If enabled it uses two queues for writes, one for the ones with disable_memtable and one for the ones that also write to memtable. 
  # This allows the memtable writes not to lag behind other writes. It can be used to optimize MySQL 2PC in which only the commits, which are serial, write to memtable. DEFAULT: false
  two_write_queues=false
  
  # Allows OS to incrementally sync files to disk while they are being written, asynchronously, in the background.
  bytes_per_sync=0

  # This is the maximum buffer size that is used by WritableFileWriter.
  writable_file_max_buffer_size=1048576
  wal_dir=tmp/ycsb-rocskdb
  
  # Specifies the time interval for the info log file to roll (in seconds). If specified with non-zero value, log file will be rolled if it has been active longer than `log_file_time_to_roll`.
  # Default: 0 (disabled)
  log_file_time_to_roll=0
  
  # Specifies the maximum number of info log files to be kept. Default: 1000
  keep_log_file_num=1000

  # DBOptionsInterface.walTtlSeconds() and DBOptionsInterface.walSizeLimitMB() affect how archived logs will be deleted.
  # If both set to 0, logs will be deleted asap and will not get into the archive.
  # If WAL_ttl_seconds is 0 and WAL_size_limit_MB is not 0, WAL files will be checked every 10 min and if total size is greater then WAL_size_limit_MB, they will be deleted starting with the earliest until size_limit is met. All empty files will be deleted.
  # If WAL_ttl_seconds is not 0 and WAL_size_limit_MB is 0, then WAL files will be checked every WAL_ttl_secondsi / 2 and those that are older than WAL_ttl_seconds will be deleted.
  # If both are not 0, WAL files will be checked every 10 min and both checks will be performed with ttl being first.
  WAL_ttl_seconds=0
  
  # This is the maximum size of all Write Buffers across all Column Families in the database. 
  # It represents the amount of data to build up in memtables across all column families before writing to disk.
  # By default this feature is disabled (by being set to 0). You should not need to change it
  db_write_buffer_size=0
  
  # Number of shards used for table cache.
  table_cache_numshardbits=6
  
  # If MutableDBOptionsInterface.maxOpenFiles() is -1, DB will open all files on DB::Open(). You can use this option to increase the number of threads used to open the files. Default: 16
  max_open_files=-1

  # If MutableDBOptionsInterface.maxOpenFiles() is -1, DB will open all files on DB::Open().
  max_file_opening_threads=16
  
  # WalTtlSeconds() and walSizeLimitMB() affect how archived logs will be deleted.
  WAL_size_limit_MB=0

  # Specifies the maximum number of concurrent background flush jobs. If you're increasing this, 
  # also consider increasing number of threads in HIGH priority thread pool. For more information, see Default: 1
  max_background_flushes=-1
  db_log_dir=

  # Specifies the maximum number of concurrent background compaction jobs, submitted to the default LOW priority thread pool. 
  # If you're increasing this, also consider increasing number of threads in LOW priority thread pool. For more information, see Default: 1
  max_background_compactions=-1
  
  # This value represents the maximum number of threads that will concurrently perform a compaction job by breaking it into multiple, 
  # smaller ones that are run simultaneously. Default: 1 (i.e. no subcompactions)
  max_subcompactions=1
  
  # Specifies the maximum number of concurrent background jobs (both flushes and compactions combined). Default: 2
  max_background_jobs=16

  # This is a maximum buffer size that is used by WinMmapReadableFile in unbuffered disk I/O mode. 
  # We need to maintain an aligned buffer for reads. We allow the buffer to grow until the specified
  # value and then for bigger requests allocate one shot buffers. In unbuffered mode we always bypass
  # read-ahead buffer at ReadaheadRandomAccessFile When read-ahead is required we then make use of 
  # MutableDBOptionsInterface.compactionReadaheadSize() value and always try to read ahead. 
  # With read-ahead we always pre-allocate buffer to the size instead of growing it up to a limit. 
  # This option is currently honored only on Windows Default: 1 Mb Special value: 0 - means do not maintain per instance buffer. 
  # Allocate per request buffer and avoid locking.
  random_access_max_buffer_size=1048576

  # The periodicity when obsolete files get deleted.
  delete_obsolete_files_period_micros=21600000000

  # If true, then DB::Open() will not update the statistics used to optimize compaction decision by loading table properties from many files.
  skip_stats_update_on_db_open=false
  skip_log_error_on_recovery=false

  # If true, then print malloc stats together with rocksdb.stats when printing to LOG. DEFAULT: false
  dump_malloc_stats=false

  # If true, the implementation will do aggressive checking of the data it is processing and will stop early if it detects any errors.
  paranoid_checks=true

  # Disable child process inherit open files.
  is_fd_close_on_exec=true

  # Manifest file is rolled over on reaching this limit. The older manifest file be deleted. 
  # The default value is MAX_INT so that roll-over does not take place.
  max_manifest_file_size=1073741824
  
  # If true, an error will be thrown during RocksDB.open() if the database already exists.
  error_if_exists=false

  # Use adaptive mutex, which spins in the user space before resorting to kernel.
  use_adaptive_mutex=false

  # If true, then the status of the threads involved in this DB will be tracked and available via GetThreadList() API. Default: false
  enable_thread_tracking=false

  # Return true if the create_missing_column_families flag is set to true.
  create_missing_column_families=false

  # Return true if the create_if_missing flag is set to true.
  create_if_missing=true

  # Number of bytes to preallocate (via fallocate) the manifest files.
  manifest_preallocation_size=4194304
  
  # Suggested number of concurrent background compaction jobs, submitted to the default LOW priority thread pool.
  base_background_compactions=-1
  
  # If true, then every store to stable storage will issue a fsync.
  use_fsync=false

  # if set to false then recovery will fail when a prepared transaction is encountered in the WAL Default: false
  allow_2pc=false
  recycle_log_file_num=0
  
  # Enable the OS to use direct reads and writes in flush and compaction Default: false
  use_direct_io_for_flush_and_compaction=false
  
  # If non-zero, we perform bigger reads when doing compaction. If you're running RocksDB on spinning disks, 
  # you should set this to at least 2MB. That way RocksDB's compaction is doing sequential instead of random reads. 
  # When non-zero, we also force DBOptionsInterface.newTableReaderForCompactionInputs() to true. Default: 0
  compaction_readahead_size=0

  # Enable the OS to use direct I/O for reading sst tables.
  use_direct_reads=false

  # Allow the OS to mmap file for writing. Default: false
  allow_mmap_writes=false
  
  # Needed to support differential snapshots. If set to true then DB will only process deletes with sequence number 
  # less than what was set by SetPreserveDeletesSequenceNumber(uint64_t ts). Clients are responsible to periodically 
  # call this method to advance the cutoff time. If this method is never called and preserve_deletes is set to true 
  # NO deletes will ever be processed. At the moment this only keeps normal deletes, SingleDeletes will not be preserved. DEFAULT: false
  preserve_deletes=false

  # If true, threads synchronizing with the write batch group leader will wait for up to DBOptionsInterface.writeThreadMaxYieldUsec() 
  # before blocking on a mutex.
  enable_write_thread_adaptive_yield=true
  
  # Returns the maximum size of a info log file.
  max_log_file_size=0

  # Whether fallocate calls are allowed
  allow_fallocate=true

  # Allow the OS to mmap file for reading. Default: false
  allow_mmap_reads=false

  # If true, always create a new file descriptor and new table reader for compaction inputs.
  new_table_reader_for_compaction_inputs=false
  
  # If set true, will hint the underlying file system that the file access pattern is random, when a sst file is opened. Default: true
  advise_random_on_open=true
  

[CFOptions "default"]
  
  # Non-bottom-level files older than TTL will go through the compaction process. This needs MutableDBOptionsInterface.maxOpenFiles() to be set to -1. Enabled only for level compaction for now. 
  # Default: 0 (disabled) Dynamically changeable through RocksDB.setOptions(ColumnFamilyHandle, MutableColumnFamilyOptions).
  ttl=0

  # kCompactionStyleLevel : Level based compaction style
  # kCompactionStyleUniversal : Universal compaction style.
  # kCompactionStyleFIFO : FIFO compaction style.
  # kCompactionStyleNone : Disable background compaction.
  compaction_style=kCompactionStyleLevel

  # This is a factory that provides AbstractCompactionFilter objects which allow an application to modify/delete a key-value during background compaction.
  compaction_filter_factory=nullptr
  
  
  # SkipList -- this is the default memtable.
  # HashSkipList -- it only makes sense with prefix_extractor. It keeps keys in buckets based on prefix of the key. Each bucket is a skip list.
  # HashLinkedList -- it only makes sense with prefix_extractor. It keeps keys in buckets based on prefix of the key. Each bucket is a linked list.
  memtable_factory=SkipListFactory
  memtable_insert_with_hint_prefix_extractor=nullptr
  
  # Set the merge operator to be used for merging two different key/value pairs that share the same key.
  merge_operator=nullptr

  prefix_extractor=nullptr

  # Use the specified comparator for key ordering.
  comparator=leveldb.BytewiseComparator
  
  # You can only choose compression types which are supported on your host system. Using compression is a trade-off between CPU, I/O and storage space.
  # Controls the compression type used for the nth level. We recommend to use ZStandard (kZSTD), or if not available, to use Zlib (kZlibCompression).
  bottommost_compression=kDisableCompressionOption
  
  # Controls the compression type used for the first n-1 levels. We recommend to use LZ4 (kLZ4Compression), or if not available, to use Snappy (kSnappyCompression).
  compression=kNoCompression
  
  # Different max-size multipliers for different levels.
  max_bytes_for_level_multiplier_additional=1:1:1:1:1:1:1
  
  # An iteration->Next() sequentially skips over keys with the same user-key unless this option is set.
  max_sequential_skip_in_iterations=8
  
  # if prefix_extractor is set and memtable_prefix_bloom_size_ratio is not 0, 
  # create prefix bloom for memtable with the size of write_buffer_size * memtable_prefix_bloom_size_ratio.
  memtable_prefix_bloom_size_ratio=0.000000
  
  # Control locality of bloom filter probes to improve cache miss rate. 
  # This option only applies to memtable prefix bloom and plaintable prefix bloom. 
  # It essentially limits the max number of cache lines each bloom filter check can touch. 
  # This optimization is turned off when set to 0. The number should never be greater than number of probes.
  # This option can boost performance for in-memory workload but should use with care since it can cause
  # higher false positive rate. Default: 0
  bloom_locality=0

  # Page size for huge page TLB for bloom in memtable. If ≤ 0, not allocate from huge page TLB but from malloc. 
  # Need to reserve huge pages for it to be allocated. For example: sysctl -w vm.nr_hugepages=20 See linux doc Documentation/vm/hugetlbpage.txt
  memtable_huge_page_size=0
  
  # Number of locks used for inplace update Default: 10000, if inplace_update_support = true, else 0.
  inplace_update_num_locks=10000
  
  # The size of one block in arena memory allocation.
  arena_block_size=16777216

  # The target file size for compaction.
  target_file_size_multiplier=1
  
  # Set the number of levels for this database If level-styled compaction is used, then this number determines the total number of levels.
  num_levels=7

  # The minimum number of write buffers that will be merged together before writing to storage. 
  # If set to 1, then all write buffers are flushed to L0 as individual files and this increases read amplification because 
  # a get request has to check in all of these files. Also, an in-memory merge may result in writing lesser data to storage 
  # if there are duplicate records in each of these individual write buffers. Default: 1
  min_write_buffer_number_to_merge=2

  # The total maximum number of write buffers to maintain in memory including copies of buffers that have already been flushed
  max_write_buffer_number_to_maintain=0
  
  # It represents the amount of data to build up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk file.
  # You need to budget for 2 x your worst case memory use. If you don't have enough memory for this, you should reduce this value.
  write_buffer_size=134217728

  # Maximum number of level-0 files.
  level0_stop_writes_trigger=36

  # Soft limit on number of level-0 files.
  level0_slowdown_writes_trigger=20

  # Number of files to trigger level-0 compaction.
  level0_file_num_compaction_trigger=2

  # The ratio between the total size of level-(L+1) files and the total size of level-L files for all L.
  max_bytes_for_level_multiplier=10.000000

  # All writes are stopped if estimated bytes needed to be compaction exceed this threshold.
  hard_pending_compaction_bytes_limit=274877906944
  
  # Maximum size of each compaction (not guarantee)
  max_compaction_bytes=1677721600

  # In debug mode, RocksDB run consistency checks on the LSM every time the LSM change (Flush, Compaction, AddFile). 
  # These checks are disabled in release mode, use this option to enable them in release mode as well. Default: false
  force_consistency_checks=false

  # This flag specifies that the implementation should optimize the filters mainly for cases where keys are found rather than also optimize for keys missed.
  optimize_filters_for_hits=false
  
  # The options for FIFO compaction style
  compaction_options_fifo={allow_compaction=false;ttl=0;max_table_files_size=1073741824;}
  
  # The upper-bound of the total size of level-1 files in bytes.
  max_bytes_for_level_base=536870912

  # Return if LevelCompactionDynamicLevelBytes is enabled.
  level_compaction_dynamic_level_bytes=false
  
  # The target file size for compaction.
  target_file_size_base=67108864

  # The maximum number of write buffers that are built up in memory.
  max_write_buffer_number=6

  # All writes will be slowed down to at least delayed_write_rate if estimated bytes needed to be compaction exceed this threshold.
  soft_pending_compaction_bytes_limit=68719476736
  
  # Allows thread-safe inplace updates. If inplace_callback function is not set, Put(key, new_value) will update inplace the,
  # existing_value iff * key exists in current memtable * new sizeof(new_value) ≤ sizeof(existing_value) * existing_value for that key is a put i.e. 
  # kTypeValue If inplace_callback function is set, check doc for inplace_callback. Default: false.
  inplace_update_support=false
  
  # Different levels can have different compression policies.
  # compression_per_level=kNoCompression:kNoCompression:kSnappyCompression:kSnappyCompression:kSnappyCompression:kSnappyCompression:kSnappyCompression
  
  # After writing every SST file, reopen it and read all the keys.
  paranoid_file_checks=false

  # Defines the table format. Here's the list of tables we support:
  #
  # Block based -- This is the default table. It is suited for storing data on disk and flash storage. 
  # It is addressed and loaded in block sized chunks (see block_size option). Thus the name block based.
  #
  # Plain Table -- Only makes sense with prefix_extractor. It is suited for storing data on memory (on tmpfs filesystem). 
  # It is byte-addressible.
  table_factory=BlockBasedTable

  # Disable automatic compactions. Manual compactions can still be issued on this column family
  disable_auto_compactions=false

  # Set the options needed to support Universal Style compactions
  compaction_options_universal={stop_style=kCompactionStopStyleTotalSize;compression_size_percent=-1;allow_trivial_move=false;max_merge_width=4294967295;max_size_amplification_percent=200;min_merge_width=2;size_ratio=1;}
  
  # A single CompactionFilter instance to call into during compaction.
  compaction_filter=nullptr

  # Compaction Priority (only applicable to leveled compaction)
  # kMinOverlappingRatio is default in RocksDB, which is optimal for most use cases. We use it in both UDB and msgdb and 
  # write amp dropped by more than half, compared to previous default
  compaction_pri=kByCompensatedSize

  # Maximum number of successive merge operations on a key in the memtable.
  max_successive_merges=0

  # Measure IO stats in compactions and flushes, if true. Default: false
  report_bg_io_stats=false
  
[TableOptions/BlockBasedTable "default"]
  # only applicable to partitioned indexes/filters. If true, the top level of the partitioned index/filter structure will
  # be pinned in the cache, regardless of the LSM tree level (that is, unlike the previous option, this affects files on 
  # all LSM tree levels, not just L0).
  pin_top_level_index_and_filter=true
  
  # Store index blocks uncompressed, which saves decompression on cache miss. It increases space for index blocks, 
  # but if index block size is small enough, overhead will be negligible.
  enable_index_compression=true

  # Number of bytes per bit to be used in block read-amp bitmap
  read_amp_bytes_per_bit=8589934592

  # format_version=2 (Up to RocksDB 5.14): The format of index and data blocks are the same, where the index blocks use same key 
  # format of <user_key,seq> but special values, <offset,size>, that point to data blocks. Different from data blocks, the option 
  # controlling restart block size is index_block_restart_interval, rather than block_restart_interval. The default value is 1, 
  # rather than 16 for data blocks. So the default is relatively memory costly. Setting the value to 8 or 16 can usually shrink index 
  # block size by half, but the CPU overhead might increase based on workloads. format_version=3,4 further optimized size, 
  # yet forward-incompatible format for index blocks.
  #
  # format_version=3 (Since RocksDB 5.15): In most of the cases the sequence number seq is not necessary for keys in the index blocks. 
  # In such cases, this format_version skips encoding the sequence number and sets index_key_is_user_key in TableProperties, which is 
  # used by the reader to know how to decode the index block.
  #
  # format_version=4 (Since RocksDB 5.16): Changes the format of index blocks by delta encoding the index values, 
  # which are the block handles. This saves the encoding of BlockHandle::offset of the non-head index entries in each restart interval. 
  # If used, TableProperties::index_value_is_delta_encoded is set, which is used by the reader to know how to decode the index block. 
  # The format of each key is (shared_size, non_shared_size, shared, non_shared). The format of each value, i.e., block handle, is 
  # (offset, size) whenever the shared_size is 0, which included the first entry in each restart point. Otherwise the format is 
  # delta-size = block handle size - size of last block handle.
  format_version=2
  index_block_restart_interval=1
  block_restart_interval=16
  
  # By default a hash of every whole key is added to the bloom filter. This can be disabled by setting this to false.
  whole_key_filtering=true
  
  # Enable/Disable align data blocks on lesser of page size and block size
  block_align=false
  metadata_block_size=4096
  
  # This is used to close a block before it reaches the configured ‘block_size’. 
  # If the percentage of free space in the current block is less than this specified number and adding a new record to the block 
  # will exceed the configured block size, then this block will be closed and the new record will be written to the next block.
  block_size_deviation=10
  partition_filters=false
  block_size=4096
  
  # Disable block cache. If this is set to true, then no block cache should be used
  no_block_cache=false
  
  # Use the specified checksum type. Newly created table files will be protected with this checksum type. 
  # Old table files will still be readable, even though they have different checksum type. Can be either crc32 or xxhash
  checksum=kCRC32c

  # kDataBlockBinarySearch traditional block type
  # kDataBlockBinaryAndHash additional hash index
  data_block_index_type=kDataBlockBinarySearch

  # binary_search a space efficient index block that is optimized for binary-search-based index.
  # hash_search the hash index. If enabled, will do hash lookup when prefix_extractor is provided.
  index_type=kBinarySearch
  
  verify_compression=false
  filter_policy=nullptr

  # Adjust The hash table utilization ratio, valid only if data_block_index_type = kDataBlockBinaryAndHash.
  data_block_hash_table_util_ratio=0.750000
  
  # Pin level-0 file's index and filter blocks in block cache, to avoid them from being evicted
  pin_l0_filter_and_index_blocks_in_cache=false

  # Set priority to high for index, filter, and compression dictionary blocks in block cache.
  cache_index_and_filter_blocks_with_high_priority=false

  # True: index and filter blocks will be stored in block cache, together with all other data blocks. This also means they can be paged out.
  # If your access pattern is very local (i.e. you have some very cold key ranges), this setting might make sense. However, in most cases it will 
  # hurt your performance, since you need to have index and filter to access a certain file. An exception is for L0 
  # when setting pin_l0_filter_and_index_blocks_in_cache=true, which can be a good compromise setting.
  #
  # False: the number of index/filter blocks is controlled by option max_open_files. If you are certain that your ulimit will always be 
  # bigger than number of files in the database, we recommend setting max_open_files to -1, which means infinity. This option will preload all 
  # filter and index blocks and will not need to maintain LRU of files. Setting max_open_files to -1 will get you the best possible performance.
  cache_index_and_filter_blocks=false
  
  # Influence the behavior when hash_search is used. If False, stores a precise prefix to block range mapping. 
  # If True, does not store prefix and allows prefix hash collision (less memory consumption)
  hash_index_allow_collision=true
  flush_block_policy_factory=FlushBlockBySizePolicyFactory
  
